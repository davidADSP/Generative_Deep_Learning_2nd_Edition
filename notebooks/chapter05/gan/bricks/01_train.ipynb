{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸ§± GANs on Bricks Data - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863c414-1946-4482-b0d0-1552ee5ffd41",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Parameters](#parameters)\n",
    "1. [Prepare the Data](#prepare)\n",
    "2. [Build the GAN](#build)\n",
    "3. [Train the GAN](#train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Dense, Reshape, Layer, BatchNormalization, LeakyReLU, ReLU, UpSampling2D, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback\n",
    "from tensorflow.keras.losses import binary_crossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.metrics import Mean, BinaryAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "from utils.image import display\n",
    "from utils.datasets import sample_batches, sample_batch\n",
    "from utils.losses import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "EPOCHS = 300\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1=0.5\n",
    "ADAM_BETA_2=0.999\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4c594-3f6d-4c8e-94c1-2c2ba7bce076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = image_dataset_from_directory(\n",
    "    \"/app/data/lego-brick-images/dataset/\",\n",
    "    labels=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation=\"bilinear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995473f-c389-4158-92d2-93a2fa937916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    Normalize and reshape the images\n",
    "    \"\"\"\n",
    "    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "train = train_data.map(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcdbdd-fb1e-451f-b89c-03fd9b80deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = sample_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53709f-7f3f-483b-9db8-2e5f9b9942c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the GAN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "x = Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(discriminator_input)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum = 0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum = 0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum = 0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias = False, activation = 'sigmoid')(x)\n",
    "discriminator_output = Flatten()(x)\n",
    "\n",
    "discriminator = Model(discriminator_input, discriminator_output)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dcc08-3869-4b67-a295-61f13d5d4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = Input(shape=(Z_DIM,))\n",
    "x = Reshape((1, 1, Z_DIM))(generator_input)\n",
    "x = Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "generator_output = Conv2DTranspose(CHANNELS, kernel_size=4, strides=2, padding=\"same\", use_bias = False, activation = 'tanh')(x)\n",
    "generator = Model(generator_input, generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed493725-488b-4390-8c64-661f3b97a632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAN(Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(GAN, self).compile()\n",
    "        self.loss_fn = BinaryCrossentropy()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_metric = Mean(name=\"d_loss\")\n",
    "        self.d_real_acc_metric = BinaryAccuracy(name=\"d_real_acc\")\n",
    "        self.d_fake_acc_metric = BinaryAccuracy(name=\"d_fake_acc\")\n",
    "        self.d_acc_metric = BinaryAccuracy(name=\"d_acc\")\n",
    "        self.g_loss_metric = Mean(name=\"g_loss\")\n",
    "        self.g_acc_metric = BinaryAccuracy(name=\"g_acc\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.d_real_acc_metric, self.d_fake_acc_metric, self.d_acc_metric, self.g_loss_metric, self.g_acc_metric]\n",
    "    \n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Train the discriminator on fake images\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            \n",
    "            generated_images = self.generator(random_latent_vectors, training = True)\n",
    "            real_predictions = self.discriminator(real_images, training = True)\n",
    "            fake_predictions = self.discriminator(generated_images, training = True)\n",
    "            \n",
    "            real_labels = tf.ones_like(real_predictions) \n",
    "            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(tf.shape(real_predictions))\n",
    "            fake_labels = tf.zeros_like(fake_predictions)\n",
    "            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(tf.shape(fake_predictions))\n",
    "            \n",
    "            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n",
    "            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2.0\n",
    "            \n",
    "            g_loss = self.loss_fn(real_labels, fake_predictions)\n",
    "            \n",
    "        \n",
    "        gradients_of_discriminator = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        gradients_of_generator = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n",
    "        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n",
    "        self.d_acc_metric.update_state([real_labels, fake_labels], [real_predictions, fake_predictions])\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.g_acc_metric.update_state(real_labels, fake_predictions)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898dd8e-f562-4517-8351-fc2f8b617a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a GAN\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=Z_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    gan.load_weights('./checkpoint/checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the GAN <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e6374-5f5b-4efa-be0a-07b182f82d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    d_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1 = ADAM_BETA_1, beta_2 = ADAM_BETA_2),\n",
    "    g_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1 = ADAM_BETA_1, beta_2 = ADAM_BETA_2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349865fe-ffbe-450e-97be-043ae1740e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "class ImageGenerator(Callback):\n",
    "    def __init__(self, num_img, latent_dim):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = generated_images * 127.5 + 127.5\n",
    "        generated_images = generated_images.numpy()\n",
    "        display(generated_images, save_to = \"./output/generated_img_%03d.png\" % (epoch))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8913a77-f472-4008-9039-dba00e6db980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gan.fit(\n",
    "    train, \n",
    "    epochs=EPOCHS, \n",
    "    # steps_per_epoch = 100, \n",
    "    # initial_epoch = 164,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, ImageGenerator(num_img=10, latent_dim=Z_DIM)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final models\n",
    "generator.save(\"./models/generator\")\n",
    "discriminator.save(\"./models/discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
