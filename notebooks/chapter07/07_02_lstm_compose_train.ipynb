{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose: Training a model to generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 22:07:18.015252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-26 22:07:18.015332: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import note, chord\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from models.RNNAttention import get_distinct, create_lookups, prepare_sequences, get_music_list, create_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "section = \"compose\"\n",
    "run_id = \"0006\"\n",
    "music_name = \"cello\"\n",
    "\n",
    "run_folder = \"run/{}/\".format(section)\n",
    "run_folder += \"_\".join([run_id, music_name])\n",
    "\n",
    "\n",
    "store_folder = os.path.join(run_folder, \"store\")\n",
    "data_folder = os.path.join(\"data\", music_name)\n",
    "\n",
    "if not os.path.exists(run_folder):\n",
    "    os.mkdir(run_folder)\n",
    "    os.mkdir(os.path.join(run_folder, \"store\"))\n",
    "    os.mkdir(os.path.join(run_folder, \"output\"))\n",
    "    os.mkdir(os.path.join(run_folder, \"weights\"))\n",
    "    os.mkdir(os.path.join(run_folder, \"viz\"))\n",
    "\n",
    "\n",
    "mode = \"build\"  # 'load' #\n",
    "\n",
    "# data params\n",
    "intervals = range(1)\n",
    "seq_len = 32\n",
    "\n",
    "# model params\n",
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"build\":\n",
    "\n",
    "    music_list, parser = get_music_list(data_folder)\n",
    "    print(len(music_list), \"files in total\")\n",
    "\n",
    "    notes = []\n",
    "    durations = []\n",
    "\n",
    "    for i, file in enumerate(music_list):\n",
    "        print(i + 1, \"Parsing %s\" % file)\n",
    "        original_score = parser.parse(file).chordify()\n",
    "\n",
    "        for interval in intervals:\n",
    "\n",
    "            score = original_score.transpose(interval)\n",
    "\n",
    "            notes.extend([\"START\"] * seq_len)\n",
    "            durations.extend([0] * seq_len)\n",
    "\n",
    "            for element in score.flat:\n",
    "\n",
    "                if isinstance(element, note.Note):\n",
    "                    if element.isRest:\n",
    "                        notes.append(str(element.name))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                    else:\n",
    "                        notes.append(str(element.nameWithOctave))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "\n",
    "                if isinstance(element, chord.Chord):\n",
    "                    notes.append(\".\".join(n.nameWithOctave for n in element.pitches))\n",
    "                    durations.append(element.duration.quarterLength)\n",
    "\n",
    "    with open(os.path.join(store_folder, \"notes\"), \"wb\") as f:\n",
    "        pickle.dump(notes, f)  # ['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open(os.path.join(store_folder, \"durations\"), \"wb\") as f:\n",
    "        pickle.dump(durations, f)\n",
    "else:\n",
    "    with open(os.path.join(store_folder, \"notes\"), \"rb\") as f:\n",
    "        notes = pickle.load(f)  # ['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open(os.path.join(store_folder, \"durations\"), \"rb\") as f:\n",
    "        durations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct sets of notes and durations\n",
    "note_names, n_notes = get_distinct(notes)\n",
    "duration_names, n_durations = get_distinct(durations)\n",
    "distincts = [note_names, n_notes, duration_names, n_durations]\n",
    "\n",
    "with open(os.path.join(store_folder, \"distincts\"), \"wb\") as f:\n",
    "    pickle.dump(distincts, f)\n",
    "\n",
    "# make the lookup dictionaries for notes and dictionaries and save\n",
    "note_to_int, int_to_note = create_lookups(note_names)\n",
    "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
    "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n",
    "\n",
    "with open(os.path.join(store_folder, \"lookups\"), \"wb\") as f:\n",
    "    pickle.dump(lookups, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nnote_to_int\")\n",
    "note_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nduration_to_int\")\n",
    "duration_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the sequences used by the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pitch input\")\n",
    "print(network_input[0][0])\n",
    "print(\"duration input\")\n",
    "print(network_input[1][0])\n",
    "print(\"pitch output\")\n",
    "print(network_output[0][0])\n",
    "print(\"duration output\")\n",
    "print(network_output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, att_model = create_network(n_notes, n_durations, embed_size, rnn_units, use_attention)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently errors in TF2.2\n",
    "# plot_model(model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = os.path.join(run_folder, \"weights\")\n",
    "# model.load_weights(os.path.join(weights_folder, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = os.path.join(run_folder, \"weights\")\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor=\"loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"), monitor=\"loss\", verbose=0, save_best_only=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", restore_best_weights=True, patience=10)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1, checkpoint2, early_stopping]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(\n",
    "    network_input,\n",
    "    network_output,\n",
    "    epochs=2000000,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
