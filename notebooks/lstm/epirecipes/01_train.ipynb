{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸ¥™ LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863c414-1946-4482-b0d0-1552ee5ffd41",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Parameters](#parameters)\n",
    "1. [Prepare the Data](#prepare)\n",
    "2. [Build the LSTM](#build)\n",
    "3. [Train the LSTM](#train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, models, callbacks, activations, losses, utils, metrics, optimizers, preprocessing, datasets\n",
    "\n",
    "from utils.image import display\n",
    "from utils.datasets import sample_batches, sample_batch\n",
    "from utils.losses import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000  # Only consider the top 20k words\n",
    "MAX_LEN = 200  # Only consider the first 200 words of each movie review\n",
    "EMBEDDING_DIM = 128\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Load the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â Load the full dataset\n",
    "with open('/app/data/epirecipes/full_format_recipes.json') as json_data:\n",
    "    recipe_data = json.load(json_data)\n",
    "    json_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04019362-8a4e-4d6d-a42f-65f9cefc8d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14557 recipes loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 09:21:28.060129: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset\n",
    "filtered_data = [x for x in recipe_data\n",
    "              if 'calories' in x \n",
    "              and x['calories'] is not None\n",
    "              and x['calories'] > 0\n",
    "              and x['calories'] < 1000\n",
    "              and 'ingredients' in x\n",
    "              and x['ingredients'] is not None\n",
    "             ]\n",
    "n_recipes = len(filtered_data)\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r' \\1 ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "filtered_data = [pad_punctuation(' '.join(x['directions'])) for x in filtered_data]\n",
    "\n",
    "filtered_data = tf.data.Dataset.from_tensor_slices(filtered_data)\n",
    "\n",
    "print(f'{n_recipes} recipes loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=string, numpy=b'Heat 3 tablespoons oil in heavy large skillet over medium heat . Add onion and next 4 ingredients . Cover ; cook until vegetables release their juices , stirring occasionally , about 12 minutes . Uncover ; saut\\xc3\\xa9 until juices evaporate , about 10 minutes . Add garlic ; cook until vegetables are very tender and just beginning to brown , about 12 minutes longer . Cool completely . Puree vegetable mixture and sour cream in processor until almost smooth . Season generously with salt and pepper . Transfer to bowl . Sprinkle with capers . Preheat broiler . Arrange bread slices on baking sheet . Brush both sides of bread slices with 1 / 3 cup oil . Toast about 2 minutes per side . Cool . Serve with spread . A combination of dried thyme , basil , savory and fennel seeds can be substituted for herbes de Provence . '>]\n"
     ]
    }
   ],
   "source": [
    "# Display an example of a recipe\n",
    "pprint(list(filtered_data.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ad49e6-b319-492a-9a6f-df53fcc7c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '.', ',', 'and', 'to', 'in', 'the', 'a', 'until', 'with', '1', 'minutes', '-', 'of', '2', 'add', 'heat', 'about', 'over']\n"
     ]
    }
   ],
   "source": [
    "# Vectorise the text into ints\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize = 'lower',\n",
    "    max_tokens=VOCAB_SIZE - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")\n",
    "\n",
    "# Adapt the mapping to the training set\n",
    "vectorize_layer.adapt(filtered_data)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices    \n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 201), dtype=int64, numpy=\n",
       "array([[  17,   30,   85,   36,    6,   67,   28,   54,   19,   26,   17,\n",
       "           2,   16,  116,    4,  539,   29,  117,    2,   50,   24,   40,\n",
       "           9,  200,  722,  749,  285,    3,   43,   91,    3,   18,  183,\n",
       "          12,    2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,\n",
       "          12,    2,   16,   87,   24,   40,    9,  200,   77,  204,   81,\n",
       "           4,   84,  431,    5,   93,    3,   18,  183,   12,  366,    2,\n",
       "          56,  199,    2,  363,  411,   25,    4,  470,   76,    6,  175,\n",
       "           9,  412,  134,    2,   68,  520,   10,   23,    4,   34,    2,\n",
       "          38,    5,   20,    2,   89,   10,  844,    2,   83,  392,    2,\n",
       "         154,  203,  157,   27,   55,  103,    2,  191,  410,  135,   14,\n",
       "         203,  157,   10,   11,   21,   30,   51,   36,    2,  450,   18,\n",
       "          15,   12,  299,  102,    2,   56,    2,   69,   10,  167,    2,\n",
       "           8, 2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,\n",
       "         220,   90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(list(filtered_data.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = filtered_data.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4818cb06-3b70-434c-b2b5-f39233897e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  17,   30,   85,   36,    6,   67,   28,   54,   19,   26,   17,\n",
       "            2,   16,  116,    4,  539,   29,  117,    2,   50,   24,   40,\n",
       "            9,  200,  722,  749,  285,    3,   43,   91,    3,   18,  183,\n",
       "           12,    2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,\n",
       "           12,    2,   16,   87,   24,   40,    9,  200,   77,  204,   81,\n",
       "            4,   84,  431,    5,   93,    3,   18,  183,   12,  366,    2,\n",
       "           56,  199,    2,  363,  411,   25,    4,  470,   76,    6,  175,\n",
       "            9,  412,  134,    2,   68,  520,   10,   23,    4,   34,    2,\n",
       "           38,    5,   20,    2,   89,   10,  844,    2,   83,  392,    2,\n",
       "          154,  203,  157,   27,   55,  103,    2,  191,  410,  135,   14,\n",
       "          203,  157,   10,   11,   21,   30,   51,   36,    2,  450,   18,\n",
       "           15,   12,  299,  102,    2,   56,    2,   69,   10,  167,    2,\n",
       "            8, 2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,\n",
       "          220,   90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>,\n",
       " <tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  30,   85,   36,    6,   67,   28,   54,   19,   26,   17,    2,\n",
       "           16,  116,    4,  539,   29,  117,    2,   50,   24,   40,    9,\n",
       "          200,  722,  749,  285,    3,   43,   91,    3,   18,  183,   12,\n",
       "            2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,   12,\n",
       "            2,   16,   87,   24,   40,    9,  200,   77,  204,   81,    4,\n",
       "           84,  431,    5,   93,    3,   18,  183,   12,  366,    2,   56,\n",
       "          199,    2,  363,  411,   25,    4,  470,   76,    6,  175,    9,\n",
       "          412,  134,    2,   68,  520,   10,   23,    4,   34,    2,   38,\n",
       "            5,   20,    2,   89,   10,  844,    2,   83,  392,    2,  154,\n",
       "          203,  157,   27,   55,  103,    2,  191,  410,  135,   14,  203,\n",
       "          157,   10,   11,   21,   30,   51,   36,    2,  450,   18,   15,\n",
       "           12,  299,  102,    2,   56,    2,   69,   10,  167,    2,    8,\n",
       "         2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,  220,\n",
       "           90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the LSTM <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 256)        263168    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 256)        394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 20000)       5140000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,357,408\n",
      "Trainable params: 8,357,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(N_UNITS, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(N_UNITS, return_sequences=True))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(VOCAB_SIZE)(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, index_to_word, max_tokens, top_k=10):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k    \n",
    "        self.word_to_index = {}\n",
    "        for index, word in enumerate(index_to_word):\n",
    "            self.word_to_index[word] = index\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "    \n",
    "    def generate(self, start_prompt):\n",
    "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = MAX_LEN - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.index_to_word[x] for x in start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.load_weights('./checkpoint/checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the LSTM <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\"adam\", loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349865fe-ffbe-450e-97be-043ae1740e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize starting prompt\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mTextGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mTextGenerator.__init__\u001b[0;34m(self, index_to_word, max_tokens, top_k)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_to_word):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mword_to_index\u001b[49m[word] \u001b[38;5;241m=\u001b[39m index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab, max_tokens = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      2\u001b[0m     text_ds, \n\u001b[1;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# steps_per_epoch = 10,\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [model_checkpoint_callback, tensorboard_callback, \u001b[43mtext_generator\u001b[49m]\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_generator' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    text_ds, \n",
    "    batch_size=32, \n",
    "    epochs=25, \n",
    "    # steps_per_epoch = 10,\n",
    "    callbacks = [model_checkpoint_callback, tensorboard_callback, text_generator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final models\n",
    "model.save(\"./models/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
