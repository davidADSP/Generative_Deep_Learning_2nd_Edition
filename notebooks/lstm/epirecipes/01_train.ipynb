{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸ¥™ LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863c414-1946-4482-b0d0-1552ee5ffd41",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Parameters](#parameters)\n",
    "1. [Prepare the Data](#prepare)\n",
    "2. [Build the LSTM](#build)\n",
    "3. [Train the LSTM](#train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, models, callbacks, activations, losses, utils, metrics, optimizers, preprocessing, datasets\n",
    "\n",
    "from utils.image import display\n",
    "from utils.datasets import sample_batches, sample_batch\n",
    "from utils.losses import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000  # Only consider the top 20k words\n",
    "MAX_LEN = 200  # Only consider the first 200 words of each movie review\n",
    "EMBEDDING_DIM = 128\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Load the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â Load the full dataset\n",
    "with open('/app/data/epirecipes/full_format_recipes.json') as json_data:\n",
    "    recipe_data = json.load(json_data)\n",
    "    json_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04019362-8a4e-4d6d-a42f-65f9cefc8d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14557 recipes loaded\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset\n",
    "filtered_data = [x for x in recipe_data\n",
    "              if 'calories' in x \n",
    "              and x['calories'] is not None\n",
    "              and x['calories'] > 0\n",
    "              and x['calories'] < 1000\n",
    "              and 'ingredients' in x\n",
    "              and x['ingredients'] is not None\n",
    "             ]\n",
    "n_recipes = len(filtered_data)\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r' \\1 ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "filtered_data = [pad_punctuation(' '.join(x['directions'])) for x in filtered_data]\n",
    "\n",
    "filtered_ds = tf.data.Dataset.from_tensor_slices(filtered_data)\n",
    "\n",
    "print(f'{n_recipes} recipes loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ad637e6-e517-4f9e-a4a6-75fe114e73a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an enamel kettle , combine the red wine , ruby port , the plum or orange brandy , lemon and orange slices , pieces of cinnamon stick , and the cloves . Bring the mixture to a simmer over low heat and simmer it for 15 minutes . Transfer the hot mulled wine mÃ©lange to a heat - proof punch bowl , rinsed with boiling water beforehand . '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=string, numpy=b'Heat 3 tablespoons oil in heavy large skillet over medium heat . Add onion and next 4 ingredients . Cover ; cook until vegetables release their juices , stirring occasionally , about 12 minutes . Uncover ; saut\\xc3\\xa9 until juices evaporate , about 10 minutes . Add garlic ; cook until vegetables are very tender and just beginning to brown , about 12 minutes longer . Cool completely . Puree vegetable mixture and sour cream in processor until almost smooth . Season generously with salt and pepper . Transfer to bowl . Sprinkle with capers . Preheat broiler . Arrange bread slices on baking sheet . Brush both sides of bread slices with 1 / 3 cup oil . Toast about 2 minutes per side . Cool . Serve with spread . A combination of dried thyme , basil , savory and fennel seeds can be substituted for herbes de Provence . '>]\n"
     ]
    }
   ],
   "source": [
    "# Display an example of a recipe\n",
    "pprint(list(filtered_ds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ad49e6-b319-492a-9a6f-df53fcc7c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '.', ',', 'and', 'to', 'in', 'the', 'a', 'until', 'with', '1', 'minutes', '-', 'of', '2', 'add', 'heat', 'about', 'over']\n"
     ]
    }
   ],
   "source": [
    "# Vectorise the text into ints\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize = 'lower',\n",
    "    max_tokens=VOCAB_SIZE - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")\n",
    "\n",
    "# Adapt the mapping to the training set\n",
    "vectorize_layer.adapt(filtered_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices    \n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 201), dtype=int64, numpy=\n",
       "array([[  17,   30,   85,   36,    6,   67,   28,   54,   19,   26,   17,\n",
       "           2,   16,  116,    4,  539,   29,  117,    2,   50,   24,   40,\n",
       "           9,  200,  722,  749,  285,    3,   43,   91,    3,   18,  183,\n",
       "          12,    2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,\n",
       "          12,    2,   16,   87,   24,   40,    9,  200,   77,  204,   81,\n",
       "           4,   84,  431,    5,   93,    3,   18,  183,   12,  366,    2,\n",
       "          56,  199,    2,  363,  411,   25,    4,  470,   76,    6,  175,\n",
       "           9,  412,  134,    2,   68,  520,   10,   23,    4,   34,    2,\n",
       "          38,    5,   20,    2,   89,   10,  844,    2,   83,  392,    2,\n",
       "         154,  203,  157,   27,   55,  103,    2,  191,  410,  135,   14,\n",
       "         203,  157,   10,   11,   21,   30,   51,   36,    2,  450,   18,\n",
       "          15,   12,  299,  102,    2,   56,    2,   69,   10,  167,    2,\n",
       "           8, 2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,\n",
       "         220,   90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(list(filtered_ds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = filtered_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4818cb06-3b70-434c-b2b5-f39233897e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  17,   30,   85,   36,    6,   67,   28,   54,   19,   26,   17,\n",
       "            2,   16,  116,    4,  539,   29,  117,    2,   50,   24,   40,\n",
       "            9,  200,  722,  749,  285,    3,   43,   91,    3,   18,  183,\n",
       "           12,    2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,\n",
       "           12,    2,   16,   87,   24,   40,    9,  200,   77,  204,   81,\n",
       "            4,   84,  431,    5,   93,    3,   18,  183,   12,  366,    2,\n",
       "           56,  199,    2,  363,  411,   25,    4,  470,   76,    6,  175,\n",
       "            9,  412,  134,    2,   68,  520,   10,   23,    4,   34,    2,\n",
       "           38,    5,   20,    2,   89,   10,  844,    2,   83,  392,    2,\n",
       "          154,  203,  157,   27,   55,  103,    2,  191,  410,  135,   14,\n",
       "          203,  157,   10,   11,   21,   30,   51,   36,    2,  450,   18,\n",
       "           15,   12,  299,  102,    2,   56,    2,   69,   10,  167,    2,\n",
       "            8, 2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,\n",
       "          220,   90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>,\n",
       " <tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  30,   85,   36,    6,   67,   28,   54,   19,   26,   17,    2,\n",
       "           16,  116,    4,  539,   29,  117,    2,   50,   24,   40,    9,\n",
       "          200,  722,  749,  285,    3,   43,   91,    3,   18,  183,   12,\n",
       "            2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,   12,\n",
       "            2,   16,   87,   24,   40,    9,  200,   77,  204,   81,    4,\n",
       "           84,  431,    5,   93,    3,   18,  183,   12,  366,    2,   56,\n",
       "          199,    2,  363,  411,   25,    4,  470,   76,    6,  175,    9,\n",
       "          412,  134,    2,   68,  520,   10,   23,    4,   34,    2,   38,\n",
       "            5,   20,    2,   89,   10,  844,    2,   83,  392,    2,  154,\n",
       "          203,  157,   27,   55,  103,    2,  191,  410,  135,   14,  203,\n",
       "          157,   10,   11,   21,   30,   51,   36,    2,  450,   18,   15,\n",
       "           12,  299,  102,    2,   56,    2,   69,   10,  167,    2,    8,\n",
       "         2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,  220,\n",
       "           90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the LSTM <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 20000)       2580000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,403,168\n",
      "Trainable params: 5,403,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
    "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(VOCAB_SIZE)(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, index_to_word, max_tokens, top_k=10):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k    \n",
    "        self.word_to_index = {}\n",
    "        for index, word in enumerate(index_to_word):\n",
    "            self.word_to_index[word] = index\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds), indices, preds \n",
    "    \n",
    "    def generate(self, start_prompt):\n",
    "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = MAX_LEN - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:MAX_LEN]\n",
    "                sample_index = MAX_LEN - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y = self.model.predict(x)\n",
    "            sample_token, indices, preds = self.sample_from(y[0][sample_index])\n",
    "            \n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "        txt = \" \".join([self.index_to_word[x] for x in start_tokens + tokens_generated])\n",
    "            \n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LOAD_MODEL:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# model.load_weights('./models/model')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/save.py:207\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaved_model_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/load.py:141\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_id, loaded_node \u001b[38;5;129;01min\u001b[39;00m keras_loader\u001b[38;5;241m.\u001b[39mloaded_nodes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    140\u001b[0m   nodes_to_load[keras_loader\u001b[38;5;241m.\u001b[39mget_path(node_id)] \u001b[38;5;241m=\u001b[39m loaded_node\n\u001b[0;32m--> 141\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m keras_loader\u001b[38;5;241m.\u001b[39mfinalize_objects()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py:842\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.saved_model.load_partial\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_partial\u001b[39m(export_dir, filters, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;124;03m\"\"\"Partially load a SavedModel (saved from V2).\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m  Similar to `tf.saved_model.load`, but with an additional argument that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m    A dictionary mapping node paths from the filter to loaded objects.\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py:974\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[1;32m    973\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_graph_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_model_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mckpt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py:149\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proto \u001b[38;5;241m=\u001b[39m object_graph_proto\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_dir \u001b[38;5;241m=\u001b[39m export_dir\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_functions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mfunction_deserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_function_def_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaved_object_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapper_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_WrapperFunction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# captures.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restored_concrete_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py:406\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# import).\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 406\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_def_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_input_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_input_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n\u001b[1;32m    412\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py:75\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001b[0m\n\u001b[1;32m     70\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m function_def_to_graph_def(\n\u001b[1;32m     71\u001b[0m     fdef, input_shapes)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     74\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m   \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_graph_def_for_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m   \u001b[38;5;66;03m# Initialize fields specific to FuncGraph.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m   \u001b[38;5;66;03m# inputs\u001b[39;00m\n\u001b[1;32m     80\u001b[0m   input_tensor_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m       nested_to_flat_tensor_name[arg\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m fdef\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39minput_arg\n\u001b[1;32m     82\u001b[0m   ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py:414\u001b[0m, in \u001b[0;36mimport_graph_def_for_function\u001b[0;34m(graph_def, name)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_graph_def_for_function\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     graph_def, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    413\u001b[0m   \u001b[38;5;124;03m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_graph_def_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_colocation_constraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py:516\u001b[0m, in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\u001b[0m\n\u001b[1;32m    504\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    506\u001b[0m   \u001b[38;5;66;03m# Create _DefinedFunctions for any imported functions.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m   \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    508\u001b[0m   \u001b[38;5;66;03m# We do this by creating _DefinedFunctions directly from `graph_def`, and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[38;5;66;03m# TODO(skyewm): fetch the TF_Functions directly from the TF_Graph\u001b[39;00m\n\u001b[1;32m    514\u001b[0m   \u001b[38;5;66;03m# TODO(skyewm): avoid sending serialized FunctionDefs back to the TF_Graph\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m   \u001b[43m_ProcessNewOps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_def\u001b[38;5;241m.\u001b[39mlibrary \u001b[38;5;129;01mand\u001b[39;00m graph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mfunction:\n\u001b[1;32m    519\u001b[0m   functions \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfrom_library(graph_def\u001b[38;5;241m.\u001b[39mlibrary)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py:250\u001b[0m, in \u001b[0;36m_ProcessNewOps\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    248\u001b[0m original_device \u001b[38;5;241m=\u001b[39m new_op\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    249\u001b[0m new_op\u001b[38;5;241m.\u001b[39m_set_device(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m colocation_names \u001b[38;5;241m=\u001b[39m \u001b[43m_GetColocationNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colocation_names:\n\u001b[1;32m    252\u001b[0m   colocation_pairs[new_op] \u001b[38;5;241m=\u001b[39m colocation_names\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py:297\u001b[0m, in \u001b[0;36m_GetColocationNames\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    295\u001b[0m colocation_names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m   class_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;66;03m# No _class attr\u001b[39;00m\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:2689\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2687\u001b[0m fields \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:\n\u001b[1;32m   2690\u001b[0m     pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_OperationGetAttrValueProto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op, name, buf)\n\u001b[1;32m   2691\u001b[0m     data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py:197\u001b[0m, in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    195\u001b[0m   buf \u001b[38;5;241m=\u001b[39m c_api\u001b[38;5;241m.\u001b[39mTF_NewBufferFromString(compat\u001b[38;5;241m.\u001b[39mas_bytes(data))\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m   buf \u001b[38;5;241m=\u001b[39m \u001b[43mc_api\u001b[49m\u001b[38;5;241m.\u001b[39mTF_NewBuffer()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m buf\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if LOAD_MODEL:\n",
    "    # model.load_weights('./models/model')\n",
    "    models.load_model('./models/model', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the LSTM <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\"adam\", loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349865fe-ffbe-450e-97be-043ae1740e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab, max_tokens = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    text_ds, \n",
    "    batch_size=32, \n",
    "    epochs=25, \n",
    "    # steps_per_epoch = 10,\n",
    "    callbacks = [model_checkpoint_callback, tensorboard_callback, text_generator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 09:39:19.925666: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3d64cde280> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3d63e32e80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3d64bd2c40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3d64bbd880> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Save the final models\n",
    "model.save(\"./models/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "heat 3                                                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_generator.generate(\"heat 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8491d7e-b5fb-45f4-a8e7-5f4916f7e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(text_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92d713d1-8f2a-4cfa-8319-9ad6eb870d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14c49f55-7d5b-4bd0-86ca-a5f388fbcdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  17,   30,   85,   36,    6,   67,   28,   54,   19,   26,   17,\n",
       "            2,   16,  116,    4,  539,   29,  117,    2,   50,   24,   40,\n",
       "            9,  200,  722,  749,  285,    3,   43,   91,    3,   18,  183,\n",
       "           12,    2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,\n",
       "           12,    2,   16,   87,   24,   40,    9,  200,   77,  204,   81,\n",
       "            4,   84,  431,    5,   93,    3,   18,  183,   12,  366,    2,\n",
       "           56,  199,    2,  363,  411,   25,    4,  470,   76,    6,  175,\n",
       "            9,  412,  134,    2,   68,  520,   10,   23,    4,   34,    2,\n",
       "           38,    5,   20,    2,   89,   10,  844,    2,   83,  392,    2,\n",
       "          154,  203,  157,   27,   55,  103,    2,  191,  410,  135,   14,\n",
       "          203,  157,   10,   11,   21,   30,   51,   36,    2,  450,   18,\n",
       "           15,   12,  299,  102,    2,   56,    2,   69,   10,  167,    2,\n",
       "            8, 2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,\n",
       "          220,   90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>,\n",
       " <tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
       " array([[  30,   85,   36,    6,   67,   28,   54,   19,   26,   17,    2,\n",
       "           16,  116,    4,  539,   29,  117,    2,   50,   24,   40,    9,\n",
       "          200,  722,  749,  285,    3,   43,   91,    3,   18,  183,   12,\n",
       "            2,  712,   24,  127,    9,  285, 2105,    3,   18,   79,   12,\n",
       "            2,   16,   87,   24,   40,    9,  200,   77,  204,   81,    4,\n",
       "           84,  431,    5,   93,    3,   18,  183,   12,  366,    2,   56,\n",
       "          199,    2,  363,  411,   25,    4,  470,   76,    6,  175,    9,\n",
       "          412,  134,    2,   68,  520,   10,   23,    4,   34,    2,   38,\n",
       "            5,   20,    2,   89,   10,  844,    2,   83,  392,    2,  154,\n",
       "          203,  157,   27,   55,  103,    2,  191,  410,  135,   14,  203,\n",
       "          157,   10,   11,   21,   30,   51,   36,    2,  450,   18,   15,\n",
       "           12,  299,  102,    2,   56,    2,   69,   10,  167,    2,    8,\n",
       "         2760,   14,  553,  321,    3,  403,    3, 2414,    4,  391,  220,\n",
       "           90,   58, 2991,   46, 2688, 1221, 2791,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bf50617-930d-491f-b517-f951525dd889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heat'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
