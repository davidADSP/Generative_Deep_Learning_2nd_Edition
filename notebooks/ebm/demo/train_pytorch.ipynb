{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ⚡️ Energy Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own Energy Based Model to predict the distribution of a demo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863c414-1946-4482-b0d0-1552ee5ffd41",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Parameters](#parameters)\n",
    "1. [Prepare the Data](#prepare)\n",
    "2. [Build the Energy Based Model](#build)\n",
    "3. [Train the Energy Based Model](#train)\n",
    "4. [Generate images](#generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f956e0-1b89-44a7-97f7-981d015ca3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150/1666516318.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"./data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./models\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "from notebooks.ebm.demo.classes import CNNModel, Sampler, DeepEnergyModel, GenerateCallback, SamplerCallback, OutlierCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUPLING_DIM = 256\n",
    "COUPLING_LAYERS = 2\n",
    "INPUT_DIM = 2\n",
    "REGULARIZATION = 0.01\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_set = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True,  drop_last=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = data.DataLoader(test_set,  batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b56a6ff7-8e01-49f7-9767-b66a633e6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"MNIST\"),\n",
    "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "                         max_epochs=60,\n",
    "                         gradient_clip_val=0.1,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='val_contrastive_divergence'),\n",
    "                                    GenerateCallback(every_n_epochs=5),\n",
    "                                    SamplerCallback(every_n_epochs=5),\n",
    "                                    OutlierCallback(),\n",
    "                                    LearningRateMonitor(\"epoch\")\n",
    "                                   ],\n",
    "                        progress_bar_refresh_rate=1)\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"MNIST.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = DeepEnergyModel.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = DeepEnergyModel(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # No testing as we are more interested in other properties\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b42522-d30d-4acd-8ae5-f8a69c027b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape=(1,28,28)\n",
    "batch_size = 128\n",
    "cnn = CNNModel()\n",
    "sampler = Sampler(cnn, img_shape=img_shape, sample_size=batch_size)\n",
    "example_input_array = torch.zeros(1, *img_shape)\n",
    "input_imgs = sampler.sample_new_exmps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23fde9cc-2b30-4956-b450-9f84406a975c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2486, -0.8098,  0.9071,  ...,  0.6993,  0.9172,  0.4312],\n",
       "          [-0.8259,  0.0522, -0.9367,  ..., -0.0095, -0.9852, -0.7568],\n",
       "          [-0.4837, -0.3348,  0.6453,  ..., -0.8984,  0.2860,  0.4679],\n",
       "          ...,\n",
       "          [-0.5985, -0.5953,  0.6912,  ..., -0.2188, -0.1369, -0.6605],\n",
       "          [ 0.5333, -0.3516,  0.6255,  ...,  0.9216,  0.6735,  0.8546],\n",
       "          [ 0.9660, -0.3096,  0.7235,  ...,  0.7404,  0.9809,  0.3164]]],\n",
       "\n",
       "\n",
       "        [[[-0.6689, -0.3632,  0.8941,  ...,  0.0585,  0.1784,  0.8069],\n",
       "          [-0.1678,  0.5656,  0.9212,  ...,  0.4515, -1.0000,  0.2704],\n",
       "          [ 0.7980, -0.6779,  0.0068,  ...,  0.8010, -0.1771, -0.7489],\n",
       "          ...,\n",
       "          [-0.3786, -0.4167,  0.1068,  ...,  0.3646, -0.9591,  0.6883],\n",
       "          [-0.1401,  0.3082,  0.9009,  ..., -0.6789,  0.1785, -0.3770],\n",
       "          [ 0.0663,  0.0209,  0.8108,  ...,  0.3384,  0.3455,  0.5107]]],\n",
       "\n",
       "\n",
       "        [[[-0.9875, -0.7701,  0.5784,  ...,  0.6955,  0.2608, -0.1162],\n",
       "          [ 0.2540,  0.0433,  0.5983,  ...,  0.0328, -0.3861, -0.5433],\n",
       "          [ 0.1709,  0.3344, -0.1519,  ...,  0.5968,  0.5919,  0.2110],\n",
       "          ...,\n",
       "          [ 0.1559,  0.1877, -0.3311,  ..., -0.5356, -0.9719, -0.2301],\n",
       "          [ 0.0587,  0.2038,  0.4979,  ...,  0.7634,  0.6656,  0.9111],\n",
       "          [ 0.5741,  0.3499,  0.2213,  ...,  0.4272,  0.5909,  0.1611]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.3871,  0.8278,  0.5969,  ...,  0.5623, -0.7880,  0.2462],\n",
       "          [ 0.9186,  0.7858,  0.4978,  ...,  0.7529,  0.2384, -0.4287],\n",
       "          [-0.6399, -0.5823, -0.9600,  ..., -0.0960,  0.9365, -0.4500],\n",
       "          ...,\n",
       "          [ 0.2653, -0.2454,  0.1835,  ..., -0.3688,  0.7980,  0.1765],\n",
       "          [ 0.1902,  0.3469, -0.3348,  ...,  0.1276,  0.3106,  0.2209],\n",
       "          [-0.8034,  0.8050,  0.4204,  ..., -0.1089, -0.5200,  0.2799]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9954, -0.3025,  0.7305,  ...,  0.5039,  0.1489, -0.6713],\n",
       "          [-0.7565, -0.3301,  0.8628,  ...,  0.5639,  0.5869, -0.8345],\n",
       "          [ 0.2692, -0.4000,  0.4597,  ...,  0.0669,  0.5781,  0.8121],\n",
       "          ...,\n",
       "          [-0.2065, -0.5960, -0.0441,  ...,  0.7521, -0.6875, -0.6468],\n",
       "          [ 0.4019,  0.6171, -0.7165,  ..., -0.0955, -0.3855, -0.0481],\n",
       "          [ 0.9496, -0.6198,  0.5597,  ..., -0.0334, -0.2892,  0.7504]]],\n",
       "\n",
       "\n",
       "        [[[-0.2921, -0.2151,  0.3864,  ...,  0.3618,  0.8067,  0.5825],\n",
       "          [-0.9011,  0.6463,  0.0476,  ...,  0.1503,  0.2136, -0.0157],\n",
       "          [ 0.3181, -0.9418,  0.2869,  ...,  0.7779,  0.8504, -0.3903],\n",
       "          ...,\n",
       "          [ 0.7327, -0.2488,  0.6117,  ..., -0.0394,  0.8001,  0.7142],\n",
       "          [-0.1740,  0.4154, -0.2016,  ..., -0.7842, -0.9394,  0.3021],\n",
       "          [-0.9684, -0.2817, -0.2737,  ..., -0.7516, -0.7457, -0.7254]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.generate_samples(sampler.model, input_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbb94b1-2dea-4a89-9e35-7808b6ff1dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampler.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd6a5a71-eb55-4ec0-9c8c-cb11a382ff90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "Missing logger folder: models/MNIST/lightning_logs\n",
      "\n",
      "  | Name | Type     | Params | In sizes       | Out sizes\n",
      "---------------------------------------------------------------\n",
      "0 | cnn  | CNNModel | 77.0 K | [1, 1, 28, 28] | [1]      \n",
      "---------------------------------------------------------------\n",
      "77.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "77.0 K    Total params\n",
      "0.308     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/utils.py:63: UserWarning: The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. Please use 'value_range' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3aedf2d74514c02b1e025188de89285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/app/notebooks/ebm/demo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     model \u001b[38;5;241m=\u001b[39m DeepEnergyModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     22\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, train_loader, test_loader)\n\u001b[0;32m---> 23\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepEnergyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# No testing as we are more interested in other properties\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hparams_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     extension \u001b[38;5;241m=\u001b[39m hparams_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/cloud_io.py:46\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\u001b[38;5;28mstr\u001b[39m(path_or_url), map_location\u001b[38;5;241m=\u001b[39mmap_location)\n\u001b[1;32m     45\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fsspec/spec.py:1037\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1037\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py:159\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py:254\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py:259\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 259\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    261\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/app/notebooks/ebm/demo'"
     ]
    }
   ],
   "source": [
    "model = train_model(img_shape=(1,28,28),\n",
    "                    batch_size=train_loader.batch_size,\n",
    "                    lr=1e-4,\n",
    "                    beta1=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f295f-ade0-4040-a6a5-a7b428b08ebc",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80087297-3f47-4e0c-ac89-8758d4386d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pl.seed_everything(43)\n",
    "callback = GenerateCallback(batch_size=4, vis_steps=8, num_steps=256)\n",
    "imgs_per_step = callback.generate_imgs(model)\n",
    "imgs_per_step = imgs_per_step.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b749-5f6e-4a12-863f-b0bbcd23549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(imgs_per_step.shape[1]):\n",
    "    step_size = callback.num_steps // callback.vis_steps\n",
    "    imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "    imgs_to_plot = torch.cat([imgs_per_step[0:1,i],imgs_to_plot], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(grid)\n",
    "    plt.xlabel(\"Generation iteration\")\n",
    "    plt.xticks([(imgs_per_step.shape[-1]+2)*(0.5+j) for j in range(callback.vis_steps+1)],\n",
    "               labels=[1] + list(range(step_size,imgs_per_step.shape[0]+1,step_size)))\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac707f6-0597-499c-9a52-7cade6724795",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rand_imgs = torch.rand((128,) + model.hparams.img_shape).to(model.device)\n",
    "    rand_imgs = rand_imgs * 2 - 1.0\n",
    "    rand_out = model.cnn(rand_imgs).mean()\n",
    "    print(f\"Average score for random images: {rand_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c52ce-e58b-49d3-b004-4101d7d8ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_imgs,_ = next(iter(train_loader))\n",
    "    train_imgs = train_imgs.to(model.device)\n",
    "    train_out = model.cnn(train_imgs).mean()\n",
    "    print(f\"Average score for training images: {train_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9981f2d-061e-4253-a3dd-005aa817a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compare_images(img1, img2):\n",
    "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
    "    score1, score2 = model.cnn(imgs).cpu().chunk(2, dim=0)\n",
    "    grid = torchvision.utils.make_grid([img1.cpu(), img2.cpu()], nrow=2, normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(grid)\n",
    "    plt.xticks([(img1.shape[2]+2)*(0.5+j) for j in range(2)],\n",
    "               labels=[\"Original image\", \"Transformed image\"])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    print(f\"Score original image: {score1:4.2f}\")\n",
    "    print(f\"Score transformed image: {score2:4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4e07e-42b1-4e4b-adf1-cca239887522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, _ = next(iter(test_loader))\n",
    "exmp_img = test_imgs[0].to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a318a-47e2-477a-b601-4dbc5221a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy = exmp_img + torch.randn_like(exmp_img) * 0.3\n",
    "img_noisy.clamp_(min=-1.0, max=1.0)\n",
    "compare_images(exmp_img, img_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbcb9c-047d-44e2-9eae-30b964a9dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_flipped = exmp_img.flip(dims=(1,2))\n",
    "compare_images(exmp_img, img_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f379bba-25ae-4af9-8430-69ae00527135",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tiny = torch.zeros_like(exmp_img)-1\n",
    "img_tiny[:,exmp_img.shape[1]//2:,exmp_img.shape[2]//2:] = exmp_img[:,::2,::2]\n",
    "compare_images(exmp_img, img_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28b266-ab35-48cd-b041-97aac36b2d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
